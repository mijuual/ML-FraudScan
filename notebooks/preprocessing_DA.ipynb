{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd3cb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id          signup_time        purchase_time  purchase_value  \\\n",
      "0    22058  2015-02-24 22:55:49  2015-04-18 02:47:11              34   \n",
      "1   333320  2015-06-07 20:39:50  2015-06-08 01:38:54              16   \n",
      "2     1359  2015-01-01 18:52:44  2015-01-01 18:52:45              15   \n",
      "3   150084  2015-04-28 21:13:25  2015-05-04 13:54:50              44   \n",
      "4   221365  2015-07-21 07:09:52  2015-09-09 18:40:53              39   \n",
      "\n",
      "       device_id source browser sex  age    ip_address  class  \n",
      "0  QVPSPJUOCKZAR    SEO  Chrome   M   39  7.327584e+08      0  \n",
      "1  EOGFQPIZPYXFZ    Ads  Chrome   F   53  3.503114e+08      0  \n",
      "2  YSSKYOSJHPPLJ    SEO   Opera   M   53  2.621474e+09      1  \n",
      "3  ATGTXKYKUDUQN    SEO  Safari   M   41  3.840542e+09      0  \n",
      "4  NAUITBZFJKHWW    Ads  Safari   M   45  4.155831e+08      0  \n",
      "user_id           0\n",
      "signup_time       0\n",
      "purchase_time     0\n",
      "purchase_value    0\n",
      "device_id         0\n",
      "source            0\n",
      "browser           0\n",
      "sex               0\n",
      "age               0\n",
      "ip_address        0\n",
      "class             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load fraud data\n",
    "fraud_df = pd.read_csv(\"../data/Fraud_Data.csv\")\n",
    "\n",
    "# Quick peek\n",
    "print(fraud_df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(fraud_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d6773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = fraud_df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Remove if found\n",
    "fraud_df = fraud_df.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d10ef41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                            int64\n",
      "signup_time               datetime64[ns]\n",
      "purchase_time             datetime64[ns]\n",
      "purchase_value                     int64\n",
      "device_id                         object\n",
      "source                            object\n",
      "browser                           object\n",
      "sex                               object\n",
      "age                                int64\n",
      "ip_address                       float64\n",
      "class                              int64\n",
      "country                           object\n",
      "user_transaction_count             int64\n",
      "prev_purchase_time                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert to datetime\n",
    "fraud_df['signup_time'] = pd.to_datetime(fraud_df['signup_time'])\n",
    "fraud_df['purchase_time'] = pd.to_datetime(fraud_df['purchase_time'])\n",
    "\n",
    "# Confirm changes\n",
    "print(fraud_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f23cc",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6735bd2",
   "metadata": {},
   "source": [
    "Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c19b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Class distribution (fraud vs non-fraud)\n",
    "sns.countplot(data=fraud_df, x='class')\n",
    "plt.title('Class Distribution (0 = Non-Fraud, 1 = Fraud)')\n",
    "plt.show()\n",
    "\n",
    "# 2. Purchase Value distribution\n",
    "sns.histplot(data=fraud_df, x='purchase_value', bins=30, kde=True)\n",
    "plt.title('Distribution of Purchase Value')\n",
    "plt.show()\n",
    "\n",
    "# 3. Age distribution\n",
    "sns.histplot(data=fraud_df, x='age', bins=30, kde=True)\n",
    "plt.title('Distribution of Age')\n",
    "plt.show()\n",
    "\n",
    "# 4. Categorical breakdown: Source, Browser, Sex\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "sns.countplot(data=fraud_df, x='source', ax=axs[0])\n",
    "sns.countplot(data=fraud_df, x='browser', ax=axs[1])\n",
    "sns.countplot(data=fraud_df, x='sex', ax=axs[2])\n",
    "plt.suptitle('Categorical Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c13c27",
   "metadata": {},
   "source": [
    "Bivariate Analysis (Features vs Fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Age vs Fraud\n",
    "sns.boxplot(data=fraud_df, x='class', y='age')\n",
    "plt.title('Age vs Fraud Class')\n",
    "plt.show()\n",
    "\n",
    "# 2. Purchase Value vs Fraud\n",
    "sns.boxplot(data=fraud_df, x='class', y='purchase_value')\n",
    "plt.title('Purchase Value vs Fraud Class')\n",
    "plt.show()\n",
    "\n",
    "# 3. Source vs Fraud Rate\n",
    "sns.barplot(data=fraud_df, x='source', y='class')\n",
    "plt.title('Fraud Rate by Source')\n",
    "plt.show()\n",
    "\n",
    "# 4. Browser vs Fraud Rate\n",
    "sns.barplot(data=fraud_df, x='browser', y='class')\n",
    "plt.title('Fraud Rate by Browser')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947458ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lower_bound_ip_address  upper_bound_ip_address    country\n",
      "0              16777216.0                16777471  Australia\n",
      "1              16777472.0                16777727      China\n",
      "2              16777728.0                16778239      China\n",
      "3              16778240.0                16779263  Australia\n",
      "4              16779264.0                16781311      China\n"
     ]
    }
   ],
   "source": [
    "ip_df = pd.read_csv(\"../data/IpAddress_to_Country.csv\")\n",
    "print(ip_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e4628",
   "metadata": {},
   "source": [
    "Convert IP Addresses to Integer Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c7c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to integer\n",
    "fraud_df['ip_address'] = fraud_df['ip_address'].astype('int')\n",
    "ip_df['lower_bound_ip_address'] = ip_df['lower_bound_ip_address'].astype('int')\n",
    "ip_df['upper_bound_ip_address'] = ip_df['upper_bound_ip_address'].astype('int')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5ab36",
   "metadata": {},
   "source": [
    "Perform the IP Range Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b48131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ip_address        country\n",
      "0  7.327584e+08          Japan\n",
      "1  3.503114e+08  United States\n",
      "2  2.621474e+09  United States\n",
      "3  3.840542e+09        Unknown\n",
      "4  4.155831e+08  United States\n"
     ]
    }
   ],
   "source": [
    "# Step: Sort IP ranges\n",
    "ip_df = ip_df.sort_values(by='lower_bound_ip_address')\n",
    "\n",
    "# Merge using interval logic\n",
    "def map_ip_to_country(user_ip):\n",
    "    match = ip_df[(ip_df['lower_bound_ip_address'] <= user_ip) & \n",
    "                  (ip_df['upper_bound_ip_address'] >= user_ip)]\n",
    "    if not match.empty:\n",
    "        return match.iloc[0]['country']\n",
    "    return 'Unknown'\n",
    "\n",
    "# Apply to fraud_df\n",
    "fraud_df['country'] = fraud_df['ip_address'].apply(map_ip_to_country)\n",
    "\n",
    "print(fraud_df[['ip_address', 'country']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836afd8",
   "metadata": {},
   "source": [
    "Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ab78252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour and day of the week\n",
    "fraud_df['hour_of_day'] = fraud_df['purchase_time'].dt.hour\n",
    "fraud_df['day_of_week'] = fraud_df['purchase_time'].dt.dayofweek  # Monday = 0, Sunday = 6\n",
    "\n",
    "# Time since signup in hours\n",
    "fraud_df['time_since_signup'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).dt.total_seconds() / 3600\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569393ff",
   "metadata": {},
   "source": [
    "Transaction Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d0c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of transactions by user\n",
    "fraud_df['user_transaction_count'] = fraud_df.groupby('user_id')['user_id'].transform('count')\n",
    "\n",
    "# Time since last transaction per user (sorted first)\n",
    "fraud_df = fraud_df.sort_values(by=['user_id', 'purchase_time'])\n",
    "fraud_df['prev_purchase_time'] = fraud_df.groupby('user_id')['purchase_time'].shift(1)\n",
    "fraud_df['time_since_last_txn'] = (fraud_df['purchase_time'] - fraud_df['prev_purchase_time']).dt.total_seconds() / 3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "410d4919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             purchase_time  hour_of_day  day_of_week  time_since_signup\n",
      "116708 2015-02-21 10:03:37           10            5         990.273333\n",
      "15108  2015-09-26 21:32:16           21            5        2788.855278\n",
      "46047  2015-08-13 11:53:07           11            3        1852.000278\n",
      "67650  2015-05-20 23:06:42           23            2         103.136111\n",
      "109067 2015-03-04 20:56:37           20            2        1286.523611\n"
     ]
    }
   ],
   "source": [
    "print(fraud_df[['purchase_time', 'hour_of_day', 'day_of_week', 'time_since_signup']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d30083",
   "metadata": {},
   "source": [
    "Apply SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48e3f0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mijuu\\Documents\\ML-FraudScan\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:558: UserWarning: Skipping features without any observed values: ['time_since_last_txn']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mijuu\\Documents\\ML-FraudScan\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:558: UserWarning: Skipping features without any observed values: ['time_since_last_txn']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE:\n",
      " class\n",
      "0    0.906352\n",
      "1    0.093648\n",
      "Name: proportion, dtype: float64\n",
      "After SMOTE:\n",
      " class\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Balanced training shape: (219136, 111584)\n",
      "Processed test shape: (30223, 111584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mijuu\\Documents\\ML-FraudScan\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:228: UserWarning: Found unknown categories in columns [0, 4] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import numpy as np\n",
    "\n",
    "# ========= Step 1: Set up your feature matrix and target =========\n",
    "X = fraud_df.drop(columns=['class'])  # or rename to match your target column\n",
    "y = fraud_df['class']\n",
    "\n",
    "# ========= Step 2: Identify columns =========\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# ========= Step 3: Train/Test Split =========\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ========= Step 4: Build preprocessing pipelines =========\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numerical_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# ========= Step 5: Combine with SMOTE in pipeline =========\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42))\n",
    "])\n",
    "\n",
    "# ========= Step 6: Fit/transform training, transform test =========\n",
    "X_train_bal, y_train_bal = pipeline.fit_resample(X_train, y_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# ========= Step 7: Output check =========\n",
    "print(\"Before SMOTE:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"After SMOTE:\\n\", pd.Series(y_train_bal).value_counts(normalize=True))\n",
    "print(\"Balanced training shape:\", X_train_bal.shape)\n",
    "print(\"Processed test shape:\", X_test_processed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11790c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove column that caused missing values warning\n",
    "if 'time_since_last_txn' in numerical_cols:\n",
    "    numerical_cols.remove('time_since_last_txn')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
